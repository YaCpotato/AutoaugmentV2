[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 16692803830800841177
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 12747311818333292269
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:1"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 13382181802757523684
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:2"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 4588267189272937326
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:3"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 11591058809388725584
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 12422795783109716932
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 6409651466208086036
physical_device_desc: "device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3d:00.0, compute capability: 7.0"
, name: "/device:GPU:1"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 13286146885390697636
physical_device_desc: "device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3e:00.0, compute capability: 7.0"
, name: "/device:GPU:2"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 4984649332131720262
physical_device_desc: "device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b1:00.0, compute capability: 7.0"
, name: "/device:GPU:3"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 9116786434374281301
physical_device_desc: "device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b2:00.0, compute capability: 7.0"
]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 128)  18560       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 128)  512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 128)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 128)  0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 128)  147584      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 128)  2176        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 128)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 128)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 128)  147584      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 32, 128)  0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 128)  147584      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 128)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 128)  147584      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 128)  512         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 128)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 32, 128)  0           activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 128)  147584      dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 128)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 128)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 128)  147584      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 128)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 128)  0           activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 128)  147584      dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 128)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 128)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 128)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 256)  295168      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 16, 16, 256)  0           activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 256)  590080      dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 256)  33024       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 256)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 256)  590080      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 16, 16, 256)  0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 256)  590080      dropout_6[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 256)  1024        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 256)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 256)  590080      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 256)  1024        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 256)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16, 16, 256)  0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 256)  590080      dropout_7[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 256)  1024        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 256)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 256)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 256)  590080      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 256)  1024        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 256)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 16, 16, 256)  0           activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 256)  590080      dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 256)  1024        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 256)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 256)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 256)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 512)    1180160     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 512)    2048        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 512)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 8, 8, 512)    0           activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 512)    2359808     dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 512)    2048        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 512)    131584      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 512)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 512)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 512)    2359808     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 512)    2048        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 512)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 8, 8, 512)    0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 512)    2359808     dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 512)    2048        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 512)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 512)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 512)    2359808     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 512)    2048        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 512)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 8, 8, 512)    0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 512)    2359808     dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 512)    2048        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 512)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 512)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 512)    2359808     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 512)    2048        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 512)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 8, 8, 512)    0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 512)    2359808     dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 512)    2048        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 512)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 512)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 512)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           5130        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 23,377,290
Trainable params: 23,362,922
Non-trainable params: 14,368
__________________________________________________________________________________________________
Train on 45000 samples, validate on 5000 samples
Epoch 1/50
 - 57s - loss: 3.4562 - acc: 0.3156 - val_loss: 1.8188 - val_acc: 0.3066
Epoch 2/50
 - 48s - loss: 1.4777 - acc: 0.4864 - val_loss: 1.9207 - val_acc: 0.3660
Epoch 3/50
 - 48s - loss: 1.2511 - acc: 0.5612 - val_loss: 1.5494 - val_acc: 0.4790
Epoch 4/50
 - 48s - loss: 1.0785 - acc: 0.6190 - val_loss: 1.3895 - val_acc: 0.5290
Epoch 5/50
 - 48s - loss: 0.9532 - acc: 0.6624 - val_loss: 1.7952 - val_acc: 0.4758
Epoch 6/50
 - 48s - loss: 0.8594 - acc: 0.6964 - val_loss: 1.1910 - val_acc: 0.5948
Epoch 7/50
 - 48s - loss: 0.7898 - acc: 0.7212 - val_loss: 1.1392 - val_acc: 0.6242
Epoch 8/50
 - 48s - loss: 0.7235 - acc: 0.7439 - val_loss: 0.9484 - val_acc: 0.6886
Epoch 9/50
 - 48s - loss: 0.6706 - acc: 0.7654 - val_loss: 0.9628 - val_acc: 0.6832
Epoch 10/50
 - 48s - loss: 0.6239 - acc: 0.7810 - val_loss: 0.8320 - val_acc: 0.7242
Epoch 11/50
 - 48s - loss: 0.5755 - acc: 0.7984 - val_loss: 1.0209 - val_acc: 0.6758
Epoch 12/50
 - 48s - loss: 0.5416 - acc: 0.8121 - val_loss: 0.9625 - val_acc: 0.6932
Epoch 13/50
 - 48s - loss: 0.5083 - acc: 0.8246 - val_loss: 0.6826 - val_acc: 0.7640
Epoch 14/50
 - 48s - loss: 0.4707 - acc: 0.8362 - val_loss: 0.7949 - val_acc: 0.7326
Epoch 15/50
 - 48s - loss: 0.4406 - acc: 0.8444 - val_loss: 0.7612 - val_acc: 0.7534
Epoch 16/50
 - 48s - loss: 0.4175 - acc: 0.8530 - val_loss: 0.7128 - val_acc: 0.7700
Epoch 17/50
 - 49s - loss: 0.3904 - acc: 0.8619 - val_loss: 1.0257 - val_acc: 0.6872
Epoch 18/50
 - 48s - loss: 0.3628 - acc: 0.8750 - val_loss: 0.8019 - val_acc: 0.7528
Epoch 19/50
 - 48s - loss: 0.3401 - acc: 0.8817 - val_loss: 0.6661 - val_acc: 0.7848
Epoch 20/50
 - 48s - loss: 0.3192 - acc: 0.8891 - val_loss: 0.6691 - val_acc: 0.7876
Epoch 21/50
 - 48s - loss: 0.3014 - acc: 0.8963 - val_loss: 0.7068 - val_acc: 0.7846
Epoch 22/50
 - 48s - loss: 0.2792 - acc: 0.9029 - val_loss: 0.9160 - val_acc: 0.7372
Epoch 23/50
 - 48s - loss: 0.2606 - acc: 0.9100 - val_loss: 0.9516 - val_acc: 0.7316
Epoch 24/50
 - 48s - loss: 0.2438 - acc: 0.9154 - val_loss: 0.7711 - val_acc: 0.7714
Epoch 25/50
 - 48s - loss: 0.2255 - acc: 0.9219 - val_loss: 0.6414 - val_acc: 0.8088
Epoch 26/50
 - 48s - loss: 0.2103 - acc: 0.9264 - val_loss: 0.6324 - val_acc: 0.8032
Epoch 27/50
 - 48s - loss: 0.1966 - acc: 0.9333 - val_loss: 0.9061 - val_acc: 0.7516
Epoch 28/50
 - 48s - loss: 0.1842 - acc: 0.9369 - val_loss: 0.7700 - val_acc: 0.7886
Epoch 29/50
 - 48s - loss: 0.1663 - acc: 0.9437 - val_loss: 0.7134 - val_acc: 0.8014
Epoch 30/50
 - 48s - loss: 0.1551 - acc: 0.9473 - val_loss: 0.9094 - val_acc: 0.7610
Epoch 31/50
 - 48s - loss: 0.1430 - acc: 0.9523 - val_loss: 0.8375 - val_acc: 0.7822
Epoch 32/50
 - 48s - loss: 0.1331 - acc: 0.9556 - val_loss: 0.7030 - val_acc: 0.8092
Epoch 33/50
 - 48s - loss: 0.1241 - acc: 0.9584 - val_loss: 0.9264 - val_acc: 0.7738
Epoch 34/50
 - 48s - loss: 0.1178 - acc: 0.9596 - val_loss: 0.9259 - val_acc: 0.7786
Epoch 35/50
 - 48s - loss: 0.1055 - acc: 0.9664 - val_loss: 0.8162 - val_acc: 0.7926
Epoch 36/50
 - 48s - loss: 0.1001 - acc: 0.9672 - val_loss: 0.7904 - val_acc: 0.8066
Epoch 37/50
 - 48s - loss: 0.0904 - acc: 0.9710 - val_loss: 0.7241 - val_acc: 0.8148
Epoch 38/50
 - 48s - loss: 0.0892 - acc: 0.9712 - val_loss: 0.9387 - val_acc: 0.7980
Epoch 39/50
 - 48s - loss: 0.0805 - acc: 0.9740 - val_loss: 0.8265 - val_acc: 0.8014
Epoch 40/50
 - 48s - loss: 0.0755 - acc: 0.9762 - val_loss: 0.8320 - val_acc: 0.8084
Epoch 41/50
 - 48s - loss: 0.0716 - acc: 0.9771 - val_loss: 0.7009 - val_acc: 0.8300
Epoch 42/50
 - 48s - loss: 0.0665 - acc: 0.9793 - val_loss: 0.7269 - val_acc: 0.8220
Epoch 43/50
 - 48s - loss: 0.0618 - acc: 0.9810 - val_loss: 0.7514 - val_acc: 0.8200
Epoch 44/50
 - 48s - loss: 0.0572 - acc: 0.9827 - val_loss: 0.8906 - val_acc: 0.7990
Epoch 45/50
 - 48s - loss: 0.0548 - acc: 0.9830 - val_loss: 0.7452 - val_acc: 0.8240
Epoch 46/50
 - 48s - loss: 0.0523 - acc: 0.9841 - val_loss: 0.9595 - val_acc: 0.7942
Epoch 47/50
 - 48s - loss: 0.0488 - acc: 0.9850 - val_loss: 0.8208 - val_acc: 0.8110
Epoch 48/50
 - 48s - loss: 0.0475 - acc: 0.9853 - val_loss: 0.8313 - val_acc: 0.8206
Epoch 49/50
 - 48s - loss: 0.0445 - acc: 0.9868 - val_loss: 0.7273 - val_acc: 0.8324
Epoch 50/50
 - 48s - loss: 0.0410 - acc: 0.9876 - val_loss: 0.8875 - val_acc: 0.8062
2427.0008862018585
--------
{'val_loss': [1.8187792278289794, 1.9206593700408936, 1.5493700157165526, 1.389478182220459, 1.795195202255249, 1.190987825012207, 1.139156010055542, 0.9483502010345459, 0.9627724464416504, 0.8319928802490234, 1.0209335065841674, 0.9624664613723755, 0.6825811365127563, 0.7948814403533936, 0.7611976501464843, 0.7128466226577759, 1.0256816366195678, 0.8019354992866516, 0.6660609661102295, 0.6691007625579835, 0.7068124443054199, 0.9160141675949097, 0.9516008556365967, 0.7710728471279145, 0.6414232288360596, 0.6324168217658996, 0.9061371828079223, 0.7699781984329224, 0.7134012084960938, 0.9093765502929687, 0.8375439682006836, 0.702965320968628, 0.9263801372528077, 0.9259194427490235, 0.8161706239700317, 0.790359986114502, 0.7240908506393433, 0.9387081207275391, 0.8264931331634522, 0.8320076515197754, 0.7008938152313232, 0.7268572217941284, 0.7513560897827148, 0.8906293928146363, 0.7452455535888672, 0.9595254276275634, 0.8208270233154297, 0.8312871404647827, 0.7273448640823365, 0.8875117908477783], 'val_acc': [0.3066, 0.366, 0.479, 0.529, 0.4758, 0.5948, 0.6242, 0.6886, 0.6832, 0.7242, 0.6758, 0.6932, 0.764, 0.7326, 0.7534, 0.77, 0.6872, 0.7528, 0.7848, 0.7876, 0.7846, 0.7372, 0.7316, 0.7714, 0.8088, 0.8032, 0.7516, 0.7886, 0.8014, 0.761, 0.7822, 0.8092, 0.7738, 0.7786, 0.7926, 0.8066, 0.8148, 0.798, 0.8014, 0.8084, 0.83, 0.822, 0.82, 0.799, 0.824, 0.7942, 0.811, 0.8206, 0.8324, 0.8062], 'loss': [3.456223035621643, 1.477734160402086, 1.2510677985933092, 1.0785287979549831, 0.953230430592431, 0.8594310778405931, 0.7897933986875746, 0.72353918961419, 0.6706183469984266, 0.6239114599757725, 0.5755304345554776, 0.541599937608507, 0.5082530663755205, 0.47066300954818724, 0.44061167431407505, 0.4175266982131534, 0.39037024513880414, 0.3628422674285041, 0.3401375220881568, 0.31918251792589825, 0.30143065695762633, 0.2792428184085422, 0.26056129474110074, 0.243786557091607, 0.2255451016108195, 0.21032072586483425, 0.1966466110944748, 0.1841758718834983, 0.1663435603949759, 0.15514687139987945, 0.14297962438530393, 0.13308040814134808, 0.12409219551351336, 0.11775626566145155, 0.1054724675556024, 0.10006680951648288, 0.09035064234468672, 0.0892171358982722, 0.08053396340873506, 0.07546898313230939, 0.07162448660201497, 0.06653837273915608, 0.06181369287437863, 0.05715061020718681, 0.054767116770148276, 0.05234678907261955, 0.04878320540984472, 0.04751701882713371, 0.044463157115711104, 0.04103173465596305], 'acc': [0.3156222222381168, 0.4863777777671814, 0.5611777777777778, 0.618977777809567, 0.6624222222646078, 0.6963555555343628, 0.7211555555767483, 0.7438888888888889, 0.7654444444338481, 0.78104444448683, 0.7983777777989706, 0.8120666667090521, 0.8246222221798367, 0.83624444448683, 0.8443555555555555, 0.85295555551317, 0.8619333333227369, 0.8750444444444444, 0.8816888888782926, 0.8890666666666667, 0.8962666666984558, 0.902933333322737, 0.9099777777353922, 0.915422222243415, 0.9219111111323038, 0.9264444444232517, 0.9333111111323039, 0.9369111110687256, 0.9436888888888889, 0.9472888889312744, 0.9523333332909478, 0.9556444444232517, 0.9584000000211927, 0.9596444444020589, 0.9663555555449592, 0.9671999999682108, 0.9710000000423855, 0.9711555555343628, 0.9739999999788073, 0.9761999999682108, 0.9770888888888889, 0.9793333333121406, 0.9809555555555556, 0.9826666666454739, 0.9830444444444445, 0.9840666666348775, 0.9850444444338481, 0.9852666666666666, 0.9868, 0.9876222222116259]}
===Final Test Score===
Test loss: 0.8874202787995339
Test accuracy: 0.8018
