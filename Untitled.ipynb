{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoaugment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート\n",
    "numpy==1.17.2  \n",
    "matplotlib==3.1.1  \n",
    "tensorflow==1.14.0  \n",
    "keras==2.2.5  \n",
    "pillow==6.1.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "from keras import models, layers, datasets, utils, backend, optimizers, initializers\n",
    "backend.set_session(session)\n",
    "from transformations import get_transformations\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# datasets in the AutoAugment paper:\n",
    "# CIFAR-10, CIFAR-100, SVHN, and ImageNet\n",
    "# SVHN = http://ufldl.stanford.edu/housenumbers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "```\n",
    "\n",
    "**tensorflowのコンフィグで、GPUの場合は必要なだけ確保。必要になり次第追加で確保するよう設定する**  \n",
    "  \n",
    "参考：[KerasでGPUメモリの使用量を抑える方法](https://qiita.com/namakemono/items/12ad8a9f6d0561929056#%E6%9C%80%E5%B0%8F%E9%99%90%E3%81%AEgpu%E3%83%A1%E3%83%A2%E3%83%AA%E3%81%AE%E3%81%BF%E7%A2%BA%E4%BF%9D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの取得関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset, reduced):\n",
    "    if dataset == 'cifar10':\n",
    "        (Xtr, ytr), (Xts, yts) = datasets.cifar10.load_data()\n",
    "    elif dataset == 'cifar100':\n",
    "        (Xtr, ytr), (Xts, yts) = datasets.cifar100.load_data()\n",
    "    else:\n",
    "        raise Exception('Unknown dataset %s' % dataset)\n",
    "    if reduced:\n",
    "        ix = np.random.choice(len(Xtr), 4000, False)\n",
    "        Xtr = Xtr[ix]\n",
    "        ytr = ytr[ix]\n",
    "    ytr = utils.to_categorical(ytr)\n",
    "    yts = utils.to_categorical(yts)\n",
    "    return (Xtr, ytr), (Xts, yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation…画像加工クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation:\n",
    "    def __init__(self, types_softmax, probs_softmax, magnitudes_softmax, argmax=False):\n",
    "        # Ekin Dogus says he sampled the softmaxes, and has not used argmax\n",
    "        # We might still want to use argmax=True for the last predictions, to ensure\n",
    "        # the best solutions are chosen and make it deterministic.\n",
    "        if argmax:\n",
    "            self.type = types_softmax.argmax()\n",
    "            t = transformations[self.type]\n",
    "            self.prob = probs_softmax.argmax() / (OP_PROBS-1)\n",
    "            m = magnitudes_softmax.argmax() / (OP_MAGNITUDES-1)\n",
    "            self.magnitude = m*(t[2]-t[1]) + t[1]\n",
    "        else:\n",
    "            self.type = np.random.choice(OP_TYPES, p=types_softmax)\n",
    "            t = transformations[self.type]\n",
    "            self.prob = np.random.choice(np.linspace(0, 1, OP_PROBS), p=probs_softmax)\n",
    "            self.magnitude = np.random.choice(np.linspace(t[1], t[2], OP_MAGNITUDES), p=magnitudes_softmax)\n",
    "        self.transformation = t[0]\n",
    "\n",
    "    def __call__(self, X):\n",
    "        _X = []\n",
    "        for x in X:\n",
    "            if np.random.rand() < self.prob:\n",
    "                x = PIL.Image.fromarray(x)\n",
    "                x = self.transformation(x, self.magnitude)\n",
    "            _X.append(np.array(x))\n",
    "        return np.array(_X)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Operation %2d (P=%.3f, M=%.3f)' % (self.type, self.prob, self.magnitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller RNNクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller:\n",
    "    def __init__(self):\n",
    "        self.model = self.create_model()\n",
    "        self.scale = tf.placeholder(tf.float32, ())\n",
    "        self.grads = tf.gradients(self.model.outputs, self.model.trainable_weights)\n",
    "        # negative for gradient ascent\n",
    "        self.grads = [g * (-self.scale) for g in self.grads]\n",
    "        self.grads = zip(self.grads, self.model.trainable_weights)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(0.00035).apply_gradients(self.grads)\n",
    "\n",
    "    def create_model(self):\n",
    "        # Implementation note: Keras requires an input. I create an input and then feed\n",
    "        # zeros to the network. Ugly, but it's the same as disabling those weights.\n",
    "        # Furthermore, Keras LSTM input=output, so we cannot produce more than SUBPOLICIES\n",
    "        # outputs. This is not desirable, since the paper produces 25 subpolicies in the\n",
    "        # end.\n",
    "        input_layer = layers.Input(shape=(SUBPOLICIES, 1))\n",
    "        init = initializers.RandomUniform(-0.1, 0.1)\n",
    "        lstm_layer = layers.LSTM(\n",
    "            LSTM_UNITS, recurrent_initializer=init, return_sequences=True,\n",
    "            name='controller')(input_layer)\n",
    "        outputs = []\n",
    "        for i in range(SUBPOLICY_OPS):\n",
    "            name = 'op%d-' % (i+1)\n",
    "            outputs += [\n",
    "                layers.Dense(OP_TYPES, activation='softmax', name=name + 't')(lstm_layer),\n",
    "                layers.Dense(OP_PROBS, activation='softmax', name=name + 'p')(lstm_layer),\n",
    "                layers.Dense(OP_MAGNITUDES, activation='softmax', name=name + 'm')(lstm_layer),\n",
    "            ]\n",
    "        return models.Model(input_layer, outputs)\n",
    "\n",
    "    def fit(self, mem_softmaxes, mem_accuracies):\n",
    "        session = backend.get_session()\n",
    "        min_acc = np.min(mem_accuracies)\n",
    "        max_acc = np.max(mem_accuracies)\n",
    "        dummy_input = np.zeros((1, SUBPOLICIES, 1))\n",
    "        dict_input = {self.model.input: dummy_input}\n",
    "        # FIXME: the paper does mini-batches (10)\n",
    "        for softmaxes, acc in zip(mem_softmaxes, mem_accuracies):\n",
    "            scale = (acc-min_acc) / (max_acc-min_acc)\n",
    "            dict_outputs = {_output: s for _output, s in zip(self.model.outputs, softmaxes)}\n",
    "            dict_scales = {self.scale: scale}\n",
    "            session.run(self.optimizer, feed_dict={**dict_outputs, **dict_scales, **dict_input})\n",
    "        return self\n",
    "\n",
    "    def predict(self, size,argmax):\n",
    "        dummy_input = np.zeros((1, size, 1), np.float32)\n",
    "        softmaxes = self.model.predict(dummy_input)\n",
    "        # convert softmaxes into subpolicies\n",
    "        subpolicies = []\n",
    "        for i in range(SUBPOLICIES):\n",
    "            operations = []\n",
    "            for j in range(SUBPOLICY_OPS):\n",
    "                op = softmaxes[j*3:(j+1)*3]\n",
    "                op = [o[0, i, :] for o in op]\n",
    "                operations.append(Operation(*op,argmax))\n",
    "            subpolicies.append(Subpolicy(*operations))\n",
    "        return softmaxes, subpolicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "def autoaugment(subpolicies, X, y):\n",
    "    while True:\n",
    "        ix = np.arange(len(X))\n",
    "        np.random.shuffle(ix)\n",
    "        for i in range(CHILD_BATCHES):\n",
    "            _ix = ix[i*CHILD_BATCH_SIZE:(i+1)*CHILD_BATCH_SIZE]\n",
    "            _X = X[_ix]\n",
    "            _y = y[_ix]\n",
    "            subpolicy = np.random.choice(subpolicies)\n",
    "            _X = subpolicy(_X)\n",
    "            _X = _X.astype(np.float32) / 255\n",
    "            yield _X, _y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Child モデルクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Child:\n",
    "    # architecture from: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "    def __init__(self, input_shape):\n",
    "        self.model = self.create_model(input_shape)\n",
    "        optimizer = optimizers.SGD(decay=1e-4)\n",
    "        self.model.compile(optimizer, 'categorical_crossentropy', ['accuracy'])\n",
    "\n",
    "    def create_model(self, input_shape):\n",
    "        x = input_layer = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "        x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(10, activation='softmax')(x)\n",
    "        return models.Model(input_layer, x)\n",
    "\n",
    "    def fit(self, subpolicies, X, y):\n",
    "        gen = autoaugment(subpolicies, X, y)\n",
    "        self.model.fit_generator(\n",
    "            gen, CHILD_BATCHES, CHILD_EPOCHS, verbose=0, use_multiprocessing=True)\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        return self.model.evaluate(X, y, verbose=0)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubPolicyクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subpolicy:\n",
    "    def __init__(self, *operations):\n",
    "        self.operations = operations\n",
    "\n",
    "    def __call__(self, X):\n",
    "        for op in self.operations:\n",
    "            X = op(X)\n",
    "        return X\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = ''\n",
    "        for i, op in enumerate(self.operations):\n",
    "            ret += str(op)\n",
    "            if i < len(self.operations)-1:\n",
    "                ret += '\\n'\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtr, ytr), (Xts, yts) = get_dataset('cifar10', True)\n",
    "transformations = get_transformations(Xtr)\n",
    "\n",
    "# Experiment parameters\n",
    "\n",
    "LSTM_UNITS = 100\n",
    "\n",
    "SUBPOLICIES = 5\n",
    "SUBPOLICY_OPS = 2\n",
    "\n",
    "OP_TYPES = 16\n",
    "OP_PROBS = 11\n",
    "OP_MAGNITUDES = 10\n",
    "\n",
    "CHILD_BATCH_SIZE = 128\n",
    "CHILD_BATCHES = len(Xtr) // CHILD_BATCH_SIZE\n",
    "CHILD_EPOCHS = 30\n",
    "CONTROLLER_EPOCHS = 50 # 15000 or 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controller: Epoch 1 / 50\n",
      "WARNING:tensorflow:From /Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "# Sub-policy 1\n",
      "Operation  5 (P=0.500, M=0.444)\n",
      "Operation 13 (P=0.600, M=1.900)\n",
      "# Sub-policy 2\n",
      "Operation  5 (P=0.300, M=0.556)\n",
      "Operation 11 (P=0.200, M=0.100)\n",
      "# Sub-policy 3\n",
      "Operation  9 (P=0.100, M=7.111)\n",
      "Operation 14 (P=0.200, M=0.133)\n",
      "# Sub-policy 4\n",
      "Operation  5 (P=0.700, M=0.444)\n",
      "Operation  7 (P=0.200, M=0.778)\n",
      "# Sub-policy 5\n",
      "Operation  1 (P=1.000, M=-0.300)\n",
      "Operation 14 (P=0.100, M=0.067)\n",
      "WARNING:tensorflow:From /Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/shoichi/Desktop/autoaugment/autobin/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mem_softmaxes = []\n",
    "mem_accuracies = []\n",
    "\n",
    "controller = Controller()\n",
    "\n",
    "for epoch in range(CONTROLLER_EPOCHS):\n",
    "    print('Controller: Epoch %d / %d' % (epoch+1, CONTROLLER_EPOCHS))\n",
    "\n",
    "    softmaxes, subpolicies = controller.predict(SUBPOLICIES,argmax=False)\n",
    "    for i, subpolicy in enumerate(subpolicies):\n",
    "        print('# Sub-policy %d' % (i+1))\n",
    "        print(subpolicy)\n",
    "    mem_softmaxes.append(softmaxes)\n",
    "\n",
    "    child = Child(Xtr.shape[1:])\n",
    "    tic = time.time()\n",
    "    child.fit(subpolicies, Xtr, ytr)\n",
    "    toc = time.time()\n",
    "    accuracy = child.evaluate(Xts, yts)\n",
    "    print('-> Child accuracy: %.3f (elaspsed time: %ds)' % (accuracy, (toc-tic)))\n",
    "    mem_accuracies.append(accuracy)\n",
    "\n",
    "    if len(mem_softmaxes) > 5:\n",
    "        # ricardo: I let some epochs pass, so that the normalization is more robust\n",
    "        controller.fit(mem_softmaxes, mem_accuracies)\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print('Best policies found:')\n",
    "print()\n",
    "_, subpolicies = controller.predict(25,argmax=True)\n",
    "for i, subpolicy in enumerate(subpolicies):\n",
    "    print('# Subpolicy %d' % (i+1))\n",
    "    print(subpolicy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
