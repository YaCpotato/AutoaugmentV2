[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 1821754608767117776
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 5119983255836940898
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:1"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 11256830792569042759
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:2"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 11800059679913946478
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:3"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 4129854160028008306
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 6150284717280261538
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 17700766294223194692
physical_device_desc: "device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3d:00.0, compute capability: 7.0"
, name: "/device:GPU:1"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 16301423347040421204
physical_device_desc: "device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3e:00.0, compute capability: 7.0"
, name: "/device:GPU:2"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 17366320132402379425
physical_device_desc: "device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b1:00.0, compute capability: 7.0"
, name: "/device:GPU:3"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 12917685771815880389
physical_device_desc: "device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b2:00.0, compute capability: 7.0"
]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 32)   4640        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 32)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 32)   544         activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 32)   0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 32, 32)   0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 32)   9248        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 32)   0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 32)   9248        add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 32)   128         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 32)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 32, 32)   0           activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 32)   9248        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 32)   128         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 32)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 32)   0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 32)   9248        add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 32)   128         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 32)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 32)   0           activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 32)   9248        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 32)   128         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 32)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 32)   0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        add_4[0][0]                      
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 32, 32, 32)   0           activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 32, 32)   9248        dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 32, 32, 32)   0           add_4[0][0]                      
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 32, 32, 32)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 32, 32, 32)   9248        dropout_6[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 32, 32, 32)   0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 32)   0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 64)   18496       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16, 16, 64)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 64)   36928       dropout_7[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 64)   2112        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 64)   0           conv2d_15[0][0]                  
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 64)   36928       add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 16, 16, 64)   0           activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 64)   0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       add_8[0][0]                      
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 16, 16, 64)   0           activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 16, 16, 64)   36928       dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 16, 16, 64)   256         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 16, 16, 64)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 16, 16, 64)   0           add_8[0][0]                      
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 16, 16, 64)   36928       add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 16, 16, 64)   256         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 16, 16, 64)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 16, 16, 64)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 16, 16, 64)   36928       dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 16, 16, 64)   256         conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 16, 16, 64)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 16, 16, 64)   0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 16, 16, 64)   36928       add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 16, 16, 64)   256         conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 16, 16, 64)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 16, 16, 64)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 16, 16, 64)   36928       dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 16, 16, 64)   256         conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 16, 16, 64)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 16, 16, 64)   0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 16, 16, 64)   36928       add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 16, 16, 64)   256         conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 16, 16, 64)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 16, 16, 64)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 16, 16, 64)   36928       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 16, 16, 64)   256         conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 16, 16, 64)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 16, 16, 64)   0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           add_12[0][0]                     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 8, 8, 128)    73856       max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 8, 8, 128)    0           activation_26[0][0]              
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 8, 8, 128)    147584      dropout_13[0][0]                 
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_30[0][0]                  
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 128)    8320        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
add_13 (Add)                    (None, 8, 8, 128)    0           conv2d_28[0][0]                  
                                                                 activation_27[0][0]              
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 8, 8, 128)    147584      add_13[0][0]                     
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 8, 8, 128)    512         conv2d_31[0][0]                  
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 8, 8, 128)    0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 8, 8, 128)    0           activation_28[0][0]              
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 8, 8, 128)    147584      dropout_14[0][0]                 
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 8, 8, 128)    512         conv2d_32[0][0]                  
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 8, 8, 128)    0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
add_14 (Add)                    (None, 8, 8, 128)    0           add_13[0][0]                     
                                                                 activation_29[0][0]              
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 8, 8, 128)    147584      add_14[0][0]                     
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 8, 8, 128)    512         conv2d_33[0][0]                  
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 8, 8, 128)    0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 8, 8, 128)    0           activation_30[0][0]              
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 8, 8, 128)    147584      dropout_15[0][0]                 
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 8, 8, 128)    512         conv2d_34[0][0]                  
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 8, 8, 128)    0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
add_15 (Add)                    (None, 8, 8, 128)    0           add_14[0][0]                     
                                                                 activation_31[0][0]              
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 8, 8, 128)    147584      add_15[0][0]                     
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 8, 8, 128)    512         conv2d_35[0][0]                  
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 8, 8, 128)    0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 8, 8, 128)    0           activation_32[0][0]              
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 8, 8, 128)    147584      dropout_16[0][0]                 
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 8, 8, 128)    512         conv2d_36[0][0]                  
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 8, 8, 128)    0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
add_16 (Add)                    (None, 8, 8, 128)    0           add_15[0][0]                     
                                                                 activation_33[0][0]              
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 8, 8, 128)    147584      add_16[0][0]                     
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 8, 8, 128)    512         conv2d_37[0][0]                  
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 8, 8, 128)    0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 8, 8, 128)    0           activation_34[0][0]              
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 8, 8, 128)    147584      dropout_17[0][0]                 
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 8, 8, 128)    512         conv2d_38[0][0]                  
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 8, 8, 128)    0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
add_17 (Add)                    (None, 8, 8, 128)    0           add_16[0][0]                     
                                                                 activation_35[0][0]              
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 8, 8, 128)    147584      add_17[0][0]                     
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 8, 8, 128)    512         conv2d_39[0][0]                  
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 8, 8, 128)    0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 8, 8, 128)    0           activation_36[0][0]              
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 8, 8, 128)    147584      dropout_18[0][0]                 
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 8, 8, 128)    512         conv2d_40[0][0]                  
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 8, 8, 128)    0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
add_18 (Add)                    (None, 8, 8, 128)    0           add_17[0][0]                     
                                                                 activation_37[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 128)          0           add_18[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           1290        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 2,251,882
Trainable params: 2,246,474
Non-trainable params: 5,408
__________________________________________________________________________________________________
Train on 45000 samples, validate on 5000 samples
Epoch 1/120
 - 27s - loss: 2.3891 - acc: 0.3017 - val_loss: 2.1597 - val_acc: 0.2612
Epoch 2/120
 - 17s - loss: 1.4910 - acc: 0.4568 - val_loss: 1.5048 - val_acc: 0.4508
Epoch 3/120
 - 17s - loss: 1.3114 - acc: 0.5243 - val_loss: 1.9399 - val_acc: 0.3946
Epoch 4/120
 - 17s - loss: 1.2012 - acc: 0.5698 - val_loss: 1.7435 - val_acc: 0.4228
Epoch 5/120
 - 17s - loss: 1.1173 - acc: 0.5995 - val_loss: 1.2254 - val_acc: 0.5660
Epoch 6/120
 - 17s - loss: 1.0482 - acc: 0.6249 - val_loss: 1.2128 - val_acc: 0.5860
Epoch 7/120
 - 17s - loss: 0.9923 - acc: 0.6434 - val_loss: 1.1636 - val_acc: 0.5834
Epoch 8/120
 - 17s - loss: 0.9456 - acc: 0.6622 - val_loss: 1.1313 - val_acc: 0.5936
Epoch 9/120
 - 17s - loss: 0.9002 - acc: 0.6811 - val_loss: 1.0869 - val_acc: 0.6110
Epoch 10/120
 - 17s - loss: 0.8701 - acc: 0.6909 - val_loss: 1.2452 - val_acc: 0.5732
Epoch 11/120
 - 17s - loss: 0.8325 - acc: 0.7050 - val_loss: 0.9764 - val_acc: 0.6612
Epoch 12/120
 - 17s - loss: 0.8042 - acc: 0.7151 - val_loss: 1.2804 - val_acc: 0.5536
Epoch 13/120
 - 17s - loss: 0.7774 - acc: 0.7236 - val_loss: 1.2156 - val_acc: 0.6092
Epoch 14/120
 - 17s - loss: 0.7545 - acc: 0.7326 - val_loss: 0.8685 - val_acc: 0.7030
Epoch 15/120
 - 17s - loss: 0.7309 - acc: 0.7411 - val_loss: 1.7968 - val_acc: 0.4506
Epoch 16/120
 - 17s - loss: 0.7109 - acc: 0.7488 - val_loss: 0.9503 - val_acc: 0.6706
Epoch 17/120
 - 17s - loss: 0.6905 - acc: 0.7563 - val_loss: 0.9577 - val_acc: 0.6726
Epoch 18/120
 - 17s - loss: 0.6705 - acc: 0.7638 - val_loss: 0.9917 - val_acc: 0.6616
Epoch 19/120
 - 17s - loss: 0.6544 - acc: 0.7702 - val_loss: 0.9605 - val_acc: 0.6698
Epoch 20/120
 - 17s - loss: 0.6344 - acc: 0.7758 - val_loss: 1.1630 - val_acc: 0.6252
Epoch 21/120
 - 17s - loss: 0.6159 - acc: 0.7834 - val_loss: 0.8078 - val_acc: 0.7214
Epoch 22/120
 - 17s - loss: 0.6032 - acc: 0.7869 - val_loss: 0.8418 - val_acc: 0.7096
Epoch 23/120
 - 17s - loss: 0.5888 - acc: 0.7925 - val_loss: 0.8440 - val_acc: 0.7266
Epoch 24/120
 - 17s - loss: 0.5710 - acc: 0.7979 - val_loss: 0.7675 - val_acc: 0.7402
Epoch 25/120
 - 17s - loss: 0.5580 - acc: 0.8040 - val_loss: 0.7132 - val_acc: 0.7594
Epoch 26/120
 - 17s - loss: 0.5432 - acc: 0.8086 - val_loss: 0.8255 - val_acc: 0.7254
Epoch 27/120
 - 17s - loss: 0.5280 - acc: 0.8144 - val_loss: 0.7226 - val_acc: 0.7586
Epoch 28/120
 - 17s - loss: 0.5164 - acc: 0.8197 - val_loss: 0.7494 - val_acc: 0.7456
Epoch 29/120
 - 17s - loss: 0.5043 - acc: 0.8229 - val_loss: 0.7464 - val_acc: 0.7576
Epoch 30/120
 - 17s - loss: 0.4940 - acc: 0.8274 - val_loss: 0.7756 - val_acc: 0.7464
Epoch 31/120
 - 17s - loss: 0.4818 - acc: 0.8299 - val_loss: 0.9269 - val_acc: 0.7002
Epoch 32/120
 - 17s - loss: 0.4727 - acc: 0.8329 - val_loss: 0.7117 - val_acc: 0.7668
Epoch 33/120
 - 17s - loss: 0.4591 - acc: 0.8373 - val_loss: 0.6664 - val_acc: 0.7822
Epoch 34/120
 - 17s - loss: 0.4523 - acc: 0.8419 - val_loss: 0.7576 - val_acc: 0.7544
Epoch 35/120
 - 17s - loss: 0.4406 - acc: 0.8448 - val_loss: 0.8511 - val_acc: 0.7328
Epoch 36/120
 - 17s - loss: 0.4335 - acc: 0.8485 - val_loss: 0.7448 - val_acc: 0.7642
Epoch 37/120
 - 17s - loss: 0.4218 - acc: 0.8524 - val_loss: 0.6999 - val_acc: 0.7796
Epoch 38/120
 - 17s - loss: 0.4142 - acc: 0.8553 - val_loss: 0.6691 - val_acc: 0.7868
Epoch 39/120
 - 17s - loss: 0.4043 - acc: 0.8602 - val_loss: 0.7067 - val_acc: 0.7818
Epoch 40/120
 - 17s - loss: 0.3983 - acc: 0.8600 - val_loss: 0.6636 - val_acc: 0.7886
Epoch 41/120
 - 17s - loss: 0.3888 - acc: 0.8635 - val_loss: 0.6962 - val_acc: 0.7776
Epoch 42/120
 - 17s - loss: 0.3809 - acc: 0.8663 - val_loss: 0.6573 - val_acc: 0.7850
Epoch 43/120
 - 17s - loss: 0.3723 - acc: 0.8697 - val_loss: 0.7075 - val_acc: 0.7810
Epoch 44/120
 - 17s - loss: 0.3660 - acc: 0.8713 - val_loss: 0.6844 - val_acc: 0.7832
Epoch 45/120
 - 17s - loss: 0.3589 - acc: 0.8733 - val_loss: 0.6358 - val_acc: 0.8016
Epoch 46/120
 - 17s - loss: 0.3530 - acc: 0.8755 - val_loss: 0.7042 - val_acc: 0.7800
Epoch 47/120
 - 17s - loss: 0.3460 - acc: 0.8774 - val_loss: 0.6871 - val_acc: 0.7854
Epoch 48/120
 - 17s - loss: 0.3359 - acc: 0.8813 - val_loss: 0.7407 - val_acc: 0.7710
Epoch 49/120
 - 17s - loss: 0.3330 - acc: 0.8848 - val_loss: 0.7085 - val_acc: 0.7874
Epoch 50/120
 - 17s - loss: 0.3299 - acc: 0.8851 - val_loss: 0.7961 - val_acc: 0.7640
Epoch 51/120
 - 17s - loss: 0.3188 - acc: 0.8892 - val_loss: 0.6529 - val_acc: 0.7982
Epoch 52/120
 - 17s - loss: 0.3128 - acc: 0.8912 - val_loss: 0.7189 - val_acc: 0.7828
Epoch 53/120
 - 17s - loss: 0.3071 - acc: 0.8915 - val_loss: 0.6398 - val_acc: 0.8042
Epoch 54/120
 - 17s - loss: 0.3044 - acc: 0.8928 - val_loss: 0.7178 - val_acc: 0.7922
Epoch 55/120
 - 17s - loss: 0.2980 - acc: 0.8964 - val_loss: 0.7807 - val_acc: 0.7714
Epoch 56/120
 - 17s - loss: 0.2921 - acc: 0.8967 - val_loss: 0.6953 - val_acc: 0.7948
Epoch 57/120
 - 17s - loss: 0.2854 - acc: 0.9002 - val_loss: 0.6381 - val_acc: 0.8074
Epoch 58/120
 - 17s - loss: 0.2851 - acc: 0.9001 - val_loss: 0.6322 - val_acc: 0.8042
Epoch 59/120
 - 17s - loss: 0.2731 - acc: 0.9040 - val_loss: 0.6901 - val_acc: 0.7978
Epoch 60/120
 - 17s - loss: 0.2772 - acc: 0.9032 - val_loss: 0.6692 - val_acc: 0.8008
Epoch 61/120
 - 17s - loss: 0.2659 - acc: 0.9069 - val_loss: 0.6651 - val_acc: 0.8040
Epoch 62/120
 - 17s - loss: 0.2636 - acc: 0.9075 - val_loss: 0.6589 - val_acc: 0.7986
Epoch 63/120
 - 17s - loss: 0.2577 - acc: 0.9099 - val_loss: 0.6608 - val_acc: 0.8038
Epoch 64/120
 - 17s - loss: 0.2509 - acc: 0.9125 - val_loss: 0.7252 - val_acc: 0.7932
Epoch 65/120
 - 17s - loss: 0.2478 - acc: 0.9130 - val_loss: 0.7280 - val_acc: 0.7892
Epoch 66/120
 - 17s - loss: 0.2436 - acc: 0.9135 - val_loss: 0.6214 - val_acc: 0.8202
Epoch 67/120
 - 17s - loss: 0.2376 - acc: 0.9168 - val_loss: 0.6801 - val_acc: 0.8064
Epoch 68/120
 - 17s - loss: 0.2332 - acc: 0.9183 - val_loss: 0.6525 - val_acc: 0.8132
Epoch 69/120
 - 17s - loss: 0.2313 - acc: 0.9179 - val_loss: 0.6362 - val_acc: 0.8158
Epoch 70/120
 - 17s - loss: 0.2246 - acc: 0.9202 - val_loss: 0.6649 - val_acc: 0.8096
Epoch 71/120
 - 17s - loss: 0.2257 - acc: 0.9208 - val_loss: 0.6836 - val_acc: 0.8068
Epoch 72/120
 - 17s - loss: 0.2169 - acc: 0.9236 - val_loss: 0.6505 - val_acc: 0.8110
Epoch 73/120
 - 17s - loss: 0.2166 - acc: 0.9249 - val_loss: 0.7239 - val_acc: 0.8016
Epoch 74/120
 - 17s - loss: 0.2116 - acc: 0.9267 - val_loss: 0.6350 - val_acc: 0.8158
Epoch 75/120
 - 17s - loss: 0.2100 - acc: 0.9254 - val_loss: 0.6917 - val_acc: 0.8070
Epoch 76/120
 - 17s - loss: 0.2092 - acc: 0.9262 - val_loss: 0.6944 - val_acc: 0.8126
Epoch 77/120
 - 17s - loss: 0.2039 - acc: 0.9278 - val_loss: 0.7218 - val_acc: 0.8036
Epoch 78/120
 - 17s - loss: 0.1953 - acc: 0.9324 - val_loss: 0.7312 - val_acc: 0.8016
Epoch 79/120
 - 17s - loss: 0.1951 - acc: 0.9314 - val_loss: 0.7211 - val_acc: 0.8028
Epoch 80/120
 - 17s - loss: 0.1920 - acc: 0.9326 - val_loss: 0.6467 - val_acc: 0.8172
Epoch 81/120
 - 17s - loss: 0.1935 - acc: 0.9319 - val_loss: 0.6837 - val_acc: 0.8158
Epoch 82/120
 - 17s - loss: 0.1866 - acc: 0.9338 - val_loss: 0.6649 - val_acc: 0.8160
Epoch 83/120
 - 17s - loss: 0.1821 - acc: 0.9370 - val_loss: 0.6685 - val_acc: 0.8160
Epoch 84/120
 - 17s - loss: 0.1806 - acc: 0.9367 - val_loss: 0.7096 - val_acc: 0.8054
Epoch 85/120
 - 17s - loss: 0.1767 - acc: 0.9382 - val_loss: 0.6701 - val_acc: 0.8134
Epoch 86/120
 - 17s - loss: 0.1727 - acc: 0.9402 - val_loss: 0.6463 - val_acc: 0.8274
Epoch 87/120
 - 17s - loss: 0.1743 - acc: 0.9397 - val_loss: 0.6994 - val_acc: 0.8116
Epoch 88/120
 - 17s - loss: 0.1703 - acc: 0.9408 - val_loss: 0.6961 - val_acc: 0.8116
Epoch 89/120
 - 17s - loss: 0.1640 - acc: 0.9436 - val_loss: 0.6972 - val_acc: 0.8146
Epoch 90/120
 - 17s - loss: 0.1645 - acc: 0.9429 - val_loss: 0.7173 - val_acc: 0.8178
Epoch 91/120
 - 17s - loss: 0.1647 - acc: 0.9431 - val_loss: 0.7460 - val_acc: 0.8104
Epoch 92/120
 - 17s - loss: 0.1600 - acc: 0.9441 - val_loss: 0.7400 - val_acc: 0.8088
Epoch 93/120
 - 17s - loss: 0.1552 - acc: 0.9454 - val_loss: 0.7803 - val_acc: 0.8008
Epoch 94/120
 - 17s - loss: 0.1550 - acc: 0.9472 - val_loss: 0.6807 - val_acc: 0.8192
Epoch 95/120
 - 17s - loss: 0.1503 - acc: 0.9476 - val_loss: 0.7123 - val_acc: 0.8110
Epoch 96/120
 - 17s - loss: 0.1492 - acc: 0.9474 - val_loss: 0.7766 - val_acc: 0.8066
Epoch 97/120
 - 17s - loss: 0.1451 - acc: 0.9490 - val_loss: 0.7416 - val_acc: 0.8010
Epoch 98/120
 - 17s - loss: 0.1440 - acc: 0.9496 - val_loss: 0.6480 - val_acc: 0.8246
Epoch 99/120
 - 17s - loss: 0.1439 - acc: 0.9494 - val_loss: 0.7375 - val_acc: 0.8146
Epoch 100/120
 - 17s - loss: 0.1419 - acc: 0.9500 - val_loss: 0.6921 - val_acc: 0.8206
Epoch 101/120
 - 17s - loss: 0.1383 - acc: 0.9526 - val_loss: 0.6967 - val_acc: 0.8240
Epoch 102/120
 - 17s - loss: 0.1348 - acc: 0.9540 - val_loss: 0.7434 - val_acc: 0.8144
Epoch 103/120
 - 17s - loss: 0.1357 - acc: 0.9530 - val_loss: 0.7008 - val_acc: 0.8242
Epoch 104/120
 - 17s - loss: 0.1350 - acc: 0.9524 - val_loss: 0.7526 - val_acc: 0.8142
Epoch 105/120
 - 17s - loss: 0.1277 - acc: 0.9565 - val_loss: 0.7155 - val_acc: 0.8128
Epoch 106/120
 - 17s - loss: 0.1284 - acc: 0.9561 - val_loss: 0.6974 - val_acc: 0.8184
Epoch 107/120
 - 17s - loss: 0.1294 - acc: 0.9562 - val_loss: 0.6947 - val_acc: 0.8244
Epoch 108/120
 - 17s - loss: 0.1267 - acc: 0.9559 - val_loss: 0.7589 - val_acc: 0.8136
Epoch 109/120
 - 17s - loss: 0.1268 - acc: 0.9567 - val_loss: 0.7251 - val_acc: 0.8202
Epoch 110/120
 - 17s - loss: 0.1217 - acc: 0.9589 - val_loss: 0.7342 - val_acc: 0.8190
Epoch 111/120
 - 17s - loss: 0.1196 - acc: 0.9592 - val_loss: 0.7258 - val_acc: 0.8186
Epoch 112/120
 - 17s - loss: 0.1185 - acc: 0.9587 - val_loss: 0.6926 - val_acc: 0.8238
Epoch 113/120
 - 17s - loss: 0.1167 - acc: 0.9606 - val_loss: 0.7047 - val_acc: 0.8272
Epoch 114/120
 - 17s - loss: 0.1151 - acc: 0.9607 - val_loss: 0.7546 - val_acc: 0.8180
Epoch 115/120
 - 17s - loss: 0.1124 - acc: 0.9613 - val_loss: 0.8052 - val_acc: 0.8074
Epoch 116/120
 - 17s - loss: 0.1118 - acc: 0.9606 - val_loss: 0.7464 - val_acc: 0.8194
Epoch 117/120
 - 17s - loss: 0.1121 - acc: 0.9619 - val_loss: 0.6720 - val_acc: 0.8306
Epoch 118/120
 - 17s - loss: 0.1083 - acc: 0.9623 - val_loss: 0.7383 - val_acc: 0.8204
Epoch 119/120
 - 17s - loss: 0.1116 - acc: 0.9610 - val_loss: 0.7677 - val_acc: 0.8158
Epoch 120/120
 - 17s - loss: 0.1079 - acc: 0.9637 - val_loss: 0.7351 - val_acc: 0.8238
2087.6620814800262
--------
{'val_loss': [2.159714539337158, 1.5047992946624755, 1.9398952308654784, 1.7435237144470215, 1.2253751970291138, 1.212782746887207, 1.163628134918213, 1.13128962726593, 1.086894474029541, 1.2452026084899903, 0.9763634336471557, 1.2804422107696534, 1.2155589347839355, 0.8685495895385742, 1.7967922882080078, 0.9503161281585694, 0.9577030113220215, 0.9917123561859131, 0.9605492778778076, 1.1630159141540528, 0.8078451112747193, 0.841824952507019, 0.8440468837738037, 0.7675153858184814, 0.7132151082992554, 0.8255439319610596, 0.7225642814636231, 0.7493615785598755, 0.7464345090866089, 0.775598291015625, 0.9269387008666993, 0.7117287952423096, 0.6663526271820068, 0.7576470109939575, 0.8510995811462402, 0.7447669296264648, 0.6998557708740234, 0.6690842845916748, 0.706707183265686, 0.663613423538208, 0.696186501121521, 0.6572670021057129, 0.7075182291030884, 0.6844076166152954, 0.6358002201080323, 0.7041748413085938, 0.6870677495956421, 0.7407019184112549, 0.7085392559051513, 0.7961402942657471, 0.6528705284118652, 0.7188507839202881, 0.6398111118316651, 0.7177601919174195, 0.7807032308578491, 0.6952582769393921, 0.6380738437652588, 0.6321865835189819, 0.6901397857666015, 0.6691509806632996, 0.6650959785461426, 0.6588567422866821, 0.660784415435791, 0.725226819896698, 0.7279758930206299, 0.6213921330451966, 0.6800659629821777, 0.65251208152771, 0.6362076412200928, 0.6649176654815674, 0.683594111251831, 0.6505341510772705, 0.7239111488342285, 0.635027951335907, 0.6916679704666138, 0.6943732971191406, 0.7218219779968261, 0.7311917869567871, 0.721149843788147, 0.6466617025375366, 0.6837465995788574, 0.6649318809509277, 0.6684615310668945, 0.7096361919403076, 0.6700876356124877, 0.6463443688392639, 0.6993659496307373, 0.6961474388122558, 0.6972424518585205, 0.7173071138381958, 0.745970749092102, 0.7400026533126831, 0.7803354780197144, 0.6806709245681762, 0.7122775074005127, 0.7766316858291626, 0.7416209182739257, 0.6479934463500977, 0.7375051635742188, 0.6921245529174804, 0.6967231185913086, 0.7433938095092774, 0.7008047077178955, 0.7526268859863281, 0.7155003681182861, 0.6974123962402343, 0.6946773262023925, 0.7589072692871094, 0.7251437103271484, 0.7342076652526855, 0.7258398561477661, 0.6926052772521972, 0.7047457300186157, 0.7545921703338623, 0.8052135091781616, 0.7463873530387879, 0.6720313071250915, 0.738287841796875, 0.7677341960906983, 0.7350905285835266], 'val_acc': [0.2612, 0.4508, 0.3946, 0.4228, 0.566, 0.586, 0.5834, 0.5936, 0.611, 0.5732, 0.6612, 0.5536, 0.6092, 0.703, 0.4506, 0.6706, 0.6726, 0.6616, 0.6698, 0.6252, 0.7214, 0.7096, 0.7266, 0.7402, 0.7594, 0.7254, 0.7586, 0.7456, 0.7576, 0.7464, 0.7002, 0.7668, 0.7822, 0.7544, 0.7328, 0.7642, 0.7796, 0.7868, 0.7818, 0.7886, 0.7776, 0.785, 0.781, 0.7832, 0.8016, 0.78, 0.7854, 0.771, 0.7874, 0.764, 0.7982, 0.7828, 0.8042, 0.7922, 0.7714, 0.7948, 0.8074, 0.8042, 0.7978, 0.8008, 0.804, 0.7986, 0.8038, 0.7932, 0.7892, 0.8202, 0.8064, 0.8132, 0.8158, 0.8096, 0.8068, 0.811, 0.8016, 0.8158, 0.807, 0.8126, 0.8036, 0.8016, 0.8028, 0.8172, 0.8158, 0.816, 0.816, 0.8054, 0.8134, 0.8274, 0.8116, 0.8116, 0.8146, 0.8178, 0.8104, 0.8088, 0.8008, 0.8192, 0.811, 0.8066, 0.801, 0.8246, 0.8146, 0.8206, 0.824, 0.8144, 0.8242, 0.8142, 0.8128, 0.8184, 0.8244, 0.8136, 0.8202, 0.819, 0.8186, 0.8238, 0.8272, 0.818, 0.8074, 0.8194, 0.8306, 0.8204, 0.8158, 0.8238], 'loss': [2.389081916194492, 1.4909571848975287, 1.3113881052441068, 1.2012148304409451, 1.117330813905928, 1.0482261738035414, 0.9922578657997979, 0.9455663107660082, 0.900186825964186, 0.8701187350591024, 0.8325175802124871, 0.8041740666813321, 0.7773807805697123, 0.7544946855545044, 0.7308744619581434, 0.7108503723568387, 0.6905362671852112, 0.6704672681278653, 0.6544102097299364, 0.6344353958235847, 0.6159386587248908, 0.6032490200360616, 0.5887778236707052, 0.5710272018750508, 0.5579778474754757, 0.5432458986494276, 0.5279594354894426, 0.5163586141798231, 0.5042790537198385, 0.49401086517969767, 0.4817966661029392, 0.47270712264378867, 0.459124871301651, 0.4523240482489268, 0.4405805168469747, 0.4334696664174398, 0.42180013323889837, 0.4142096386803521, 0.40427208893564015, 0.39828168608347575, 0.3888213622464074, 0.3809265425682068, 0.3722737589756648, 0.36596670073403254, 0.35890105613072715, 0.3529752431074778, 0.3459623794767592, 0.33586770154105294, 0.332989548974567, 0.32985402580632106, 0.3187962482240465, 0.3128179780006409, 0.30714058995776705, 0.30435591300328574, 0.297999613544676, 0.29211035907268523, 0.28541843353907265, 0.2850798783461253, 0.27308564150068493, 0.27723835413191056, 0.2658834528658125, 0.26362751649220784, 0.2577025209373898, 0.2508829893694984, 0.2478226608832677, 0.2435707884311676, 0.23763207443555195, 0.2332286507924398, 0.23127730773289998, 0.22463708690007528, 0.22571199931038752, 0.21694834095901913, 0.21662997855610316, 0.21159309407075247, 0.2099822931130727, 0.2091786614788903, 0.20394941951963635, 0.19526800598038568, 0.19514066155619092, 0.1919797079510159, 0.19348435159259372, 0.18657600359916687, 0.18211382269859314, 0.18062654559612273, 0.176689428085751, 0.17265112692515056, 0.174270081782341, 0.17028308412101534, 0.16404703270594279, 0.16452205827501085, 0.16472008868323432, 0.16000880253050062, 0.15519980284372967, 0.15496224290927252, 0.15026305063035753, 0.14920841844081878, 0.14512113398313523, 0.14400987499819862, 0.14391608721415203, 0.14190083112981583, 0.13833361308839587, 0.1347684369881948, 0.13573943619728088, 0.13495956587526534, 0.1277309291627672, 0.12836565209892062, 0.1293713531865014, 0.1266968307932218, 0.12676463720533582, 0.12172627229425642, 0.11964734117653635, 0.11852173913584815, 0.1166608992709054, 0.11509573322667016, 0.11239923364321391, 0.11179828864865833, 0.11205151722298728, 0.10828523115979301, 0.11159584142764409, 0.10792261377440558], 'acc': [0.3017111111111111, 0.45682222220102947, 0.5242888889312745, 0.5698222222222222, 0.5995111111217075, 0.6249111111323039, 0.6434222222328186, 0.6621999999682109, 0.6811111111217075, 0.6909333333651225, 0.7049999999682108, 0.7150888888676962, 0.7236000000423856, 0.7325777777459886, 0.7410666666348775, 0.7488444444232517, 0.7562666666242811, 0.763822222243415, 0.7701555555237665, 0.7757555555873447, 0.7834, 0.7869111110687256, 0.7924666666560702, 0.7979111111429003, 0.8040444444762336, 0.8086222222328187, 0.8143555555343628, 0.8196888888782925, 0.8228888888888889, 0.8274444444126553, 0.8298666666984558, 0.8329333333651224, 0.837333333322737, 0.8418666666666667, 0.8447555555661519, 0.8484888888782925, 0.8523777777989705, 0.8553111111323038, 0.8602222222328186, 0.8600444444338481, 0.8634888888782926, 0.866333333322737, 0.8696666666242812, 0.8712666666560703, 0.8732888888465034, 0.8755111111111111, 0.8773555555555556, 0.881333333322737, 0.884844444433848, 0.8851333333757189, 0.8892000000423855, 0.8911777777353923, 0.891466666677263, 0.8928000000211928, 0.8964222222116258, 0.896688888920678, 0.9001555555767483, 0.9001111111534966, 0.9039555555661519, 0.9031777777671814, 0.9069111111429002, 0.9074888888994853, 0.9099111111534967, 0.9125111111217075, 0.9129777778095669, 0.9135111111323039, 0.9168444444762336, 0.9182666666242811, 0.9178888889100817, 0.9202444444550408, 0.9207555555979411, 0.9235555555767483, 0.9249111111323038, 0.9266888889312744, 0.9254000000317891, 0.9261999999576145, 0.9278444444020589, 0.932377777756585, 0.931422222190433, 0.9326222222328187, 0.931911111079322, 0.933777777809567, 0.9369777777883742, 0.9367333333545261, 0.9381555555873446, 0.9401555555767483, 0.9397333333651224, 0.9408444444444445, 0.9436222222540114, 0.9428888888465033, 0.9431333333651225, 0.9441111110899183, 0.9453777778201633, 0.9472444444232517, 0.9475555555131701, 0.9473777778095669, 0.9490222221904331, 0.9496444444126553, 0.9493555555131701, 0.9500222221904331, 0.9525777778201633, 0.9539555555979411, 0.9529555555873447, 0.9523777777459886, 0.9565333332909478, 0.9560888889312744, 0.9561777777459887, 0.955933333322737, 0.9567111111429003, 0.9588666666348775, 0.959244444433848, 0.9587333333121406, 0.9605555555449592, 0.9607333333651225, 0.9612666666348775, 0.960577777756585, 0.9618666666560702, 0.9623333332909478, 0.9609777777671814, 0.9637333333015442]}
===Final Test Score===
Test loss: 0.7367926131486893
Test accuracy: 0.8139
